---
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\pagenumbering{gobble}

```{r warning=FALSE, message=F,echo=F}
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
### matrix termo doc
if(!require(tm)){install.packages("tm")}
library(tm)
### operacoes na matrix termo-doc
if(!require(slam)){install.packages("slam")}
library(slam)
### bibliotecas de stopwords
if(!require(stopwords)){install.packages("stopwords")}
library(stopwords)
### bibliotecas de stopwords
if(!require(SnowballC)){install.packages("SnowballC")}
library(SnowballC)
### nuvem de palavras
if(!require(wordcloud)){install.packages("wordcloud")}
library(wordcloud)
### bibliotecas de stopwords
if(!require(RColorBrewer)){install.packages("RColorBrewer")}
library(RColorBrewer)
### grafico
if(!require(ggplot2)){install.packages("ggplot2")}
library(ggplot2)
if(!require(ggpubr)){install.packages("ggpubr")}
library(ggpubr)
### Matriz esparsa geral
if(!require(Matrix)){install.packages("Matrix")}
library(Matrix)
### Ponto de corte optimo
if(!require(pROC)){install.packages("pROC")}
library(pROC)
### XGB
if(!require(xgboost)){install.packages("xgboost")}
library(xgboost)
```

## Predizendo o banco de dados não rotulado

\quad Para a predição do banco de dados não rotulado, escolheu-se o modelo XGBoost para cada tag (com o número de árvores igual a 100), isto é o modelo 10. Em relação ao banco de dados, utilizou-se aquele com a matriz documento texto em que havia o título e a sinopse juntos, pegando apenas as palavras que apareciam mais de 5% das vezes.

\quad Mas dessa vez, para o ajuste do modelo, utilizou-se 100% do conjunto de dados rotulados, de forma a obter o máximo de informação possível.

\quad Com base nessa matriz documento termo do treino, criou-se outra matriz documento termo, mas agora para os dados não rotulados. Dessa forma, o dtm dos dados não rotulados possuirá apenas as palavras com quais o modelo foi treinado. 




```{r message=FALSE, warning=FALSE,echo=F}
library(readr)
labeled_data <- read_csv("labeled_data.csv")
unlabeled_data <- read_csv("unlabeled_data.csv")

dados = labeled_data[,3:84]
dados$texto = paste(labeled_data$title,
                    labeled_data$plot_synopsis)

resumo = VCorpus(VectorSource(c(dados$texto)),
                 readerControl = list(languague = "en"))

resumo1 =  tm_map(resumo, 
                 removeWords, 
                 stopwords(language = "en",source = "smart"))

dtm = resumo1 %>%
  DocumentTermMatrix(control = list(tolower=T,
                                    removePunctuation = T,
                                    removeNumbers = T,
                                    stripWhitespace = T,
                                    stopwords = T,
                                    stemming = T,
                                    weighting= weightTfIdf))

dtm2 = removeSparseTerms(dtm,0.95)
dtm2
```

\quad Desse modo, nota-se que com o dtm2 (conjunto rotulado) possui uma esparcidade razoável e uma boa quantidade de termos e filmes.

\quad Cria-se o dtm do conjunto de dados não rotulado.

```{r message=F,warning=F}
resumo = VCorpus(VectorSource(c(paste(unlabeled_data$title,
                                      unlabeled_data$plot_synopsis))),
                 readerControl = list(languague = "en"))

resumo1 =  tm_map(resumo, 
                 removeWords, 
                 stopwords(language = "en",source = "smart"))

dtm_teste_unlabeled = resumo1 %>%
  DocumentTermMatrix(control = list(tolower=T,
                                    removePunctuation = T,
                                    removeNumbers = T,
                                    stripWhitespace = T,
                                    stopwords = T,
                                    stemming = T,
                                    weighting= weightTfIdf,
                                    ### Pega-se apenas as palavras do rotulado
                                    dictionary=Terms(dtm2)))
dtm_teste_unlabeled
```

\quad Nota-se novamente uma esparsidade razoável. Mas agora há 5000 filmes (número de filmes dos dados não rotulados), e com as mesmas palavras (covariáveis), que foram 1033. Em seguida, treina-se o modelo escolhido.

```{r message=FALSE, warning=FALSE,echo=F,eval=F}
library(readr)
#labeled_data <- read_csv("D:/Mega/ufscar_mega/Ufscar/7° ENPE #3/MD/lista3/labeled_data.csv")
labeled_data <- read_csv("labeled_data.csv")

#labeled_data <- read_csv("UFSCAR_Cina/MD/lista3/labeled_data.csv")

#unlabeled_data <- read_csv("D:/Mega/ufscar_mega/Ufscar/7° ENPE #3/MD/lista3/unlabeled_data.csv")

unlabeled_data <- read_csv("unlabeled_data.csv")
#unlabeled_data=read_csv("UFSCAR_Cina/MD/lista3/unlabeled_data.csv")

dados = labeled_data[,3:84]
dados$texto = paste(labeled_data$title,
                    labeled_data$plot_synopsis)
```


```{r message=FALSE,warning=FALSE}
lista_modelos_xgb_final = list()

for(i in 1:ncol(dados[,-83]) ){
  
  xgb_mat_treino_tudo = 
    xgb.DMatrix(as.matrix(dtm2),
                label = as.matrix(dados[,i]))
  
  set.seed(i+500)
  #cat('A coluna eh ',i,'\n')
  
  modelo_xgb =
  xgboost(data = xgb_mat_treino_tudo,
     nrounds = 100,
     eval_metric = "auc",
     verbose = 0,
     objective = "binary:logistic")

  lista_modelos_xgb_final[[i]] = modelo_xgb
}
```

\quad Com os micro modelos XGboost para cada tag criados, basta predizer as probabilidades no banco de dados não rotulado.

```{r message=F,warning=F}
mat_prob_unlabeled = matrix(NA, 
                            nrow=nrow(dtm_teste_unlabeled),
                            ncol = ncol(dados[,-83]) )

for(i in 1:ncol(dados[,-83]) ){
  
  mat_prob_unlabeled[,i] <- 
    predict(lista_modelos_xgb_final[[i]],
            xgb.DMatrix(as.matrix(dtm_teste_unlabeled)),
            ype="prob")

}
```

\quad Com isso, tem-se uma matriz que representa as probabilidades de cada filme possuir determinadas tags. Para continuar a análise, é preciso estabelecer um ponto de corte de modo a obter 1 (se a probabilidade for maior) e 0 (se a probabilidade for menor).

\quad Para tal, escolhe-se a proporção de tags no banco de dados rotulado (semelhante ao ponto de corte usado para a métrica de F1 dos modelos anteriores, sendo tal métrica baseada na proporção das tags).


```{r message=F,warning=F}
unlabeled_data_predito = unlabeled_data

unlabeled_data_predito[,3:(ncol(dados[,-83])+2)] = 
  ifelse(mat_prob_unlabeled>
           matrix(rep(colMeans(dados[,-83]),nrow(dtm_teste_unlabeled)),
                  byrow = T, 
                  ncol = ncol(dados[,-83]),
                  nrow = nrow(dtm_teste_unlabeled) ),
         1,0)

unlabeled_data_predito[1:5,1:5]
```

\quad Com isso, tem-se os dados não rotulados agora rotulados, com valores 1 (se tag está presente) e 0 (se tag não está presente). Basta agora exportar tal tabela como um CSV.

```{r message=F,warning=F}
library(readr)
write_csv(unlabeled_data_predito,file="LubenLuizVinicius.csv")
```




